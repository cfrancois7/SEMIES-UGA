{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888cad2e",
   "metadata": {},
   "source": [
    "# Extract main content from contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e236bba",
   "metadata": {},
   "source": [
    "We based the methodology to the impressive feature of LLM to reformulate the textual content.\n",
    "The propose approach is to use LLM to reformulate the contribution as a list of main ideas.\n",
    "More specifically, we use the LLM to reformulate the contributions as a set of opinions and propositions.\n",
    "\n",
    "**WARNING**:\n",
    "The code below works for CUDA compatible GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97794da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# to free the memory if already mounted model in GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acddc2",
   "metadata": {},
   "source": [
    "## Load the LLM model and its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67907030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_REPO = Path(\"../config\").resolve()\n",
    "CONFIG_PATH = CONFIG_REPO / \"llama-3.1-8B-FR.toml\"\n",
    "PROMPT_PATH = CONFIG_REPO / \"prompt.toml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"rb\") as file:\n",
    "    configs = tomllib.load(file)\n",
    "\n",
    "model_id = configs[\"model\"][\"name\"]\n",
    "top_p = configs[\"model\"][\"top_p\"]\n",
    "temperature = configs[\"model\"][\"temperature\"]\n",
    "sampling_params = dict(top_p=top_p, temperature=temperature, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7379fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine_learning/projects/perspectiva/prisma-logic/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 20:34:30 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 20:34:32,722\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 20:34:45 [awq_marlin.py:113] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 05-11 20:34:45 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=3072.\n",
      "INFO 05-11 20:34:46 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-11 20:34:47 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5af6e4c140>\n",
      "INFO 05-11 20:34:49 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "WARNING 05-11 20:34:49 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-11 20:34:49 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-11 20:34:49 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 05-11 20:34:49 [gpu_model_runner.py:1329] Starting to load model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4...\n",
      "INFO 05-11 20:34:50 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:11<00:11, 11.95s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 24.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 22.61s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 20:35:35 [loader.py:458] Loading weights took 45.32 seconds\n",
      "INFO 05-11 20:35:37 [gpu_model_runner.py:1347] Model loading took 5.3744 GiB and 47.451011 seconds\n",
      "INFO 05-11 20:35:51 [backends.py:420] Using cache directory: /home/machine_learning/.cache/vllm/torch_compile_cache/9893167dc5/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-11 20:35:51 [backends.py:430] Dynamo bytecode transform time: 13.86 s\n",
      "INFO 05-11 20:35:57 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 4.761 s\n",
      "INFO 05-11 20:35:58 [monitor.py:33] torch.compile takes 13.86 s in total\n",
      "INFO 05-11 20:36:02 [kv_cache_utils.py:634] GPU KV cache size: 5,824 tokens\n",
      "INFO 05-11 20:36:02 [kv_cache_utils.py:637] Maximum concurrency for 3,072 tokens per request: 1.90x\n",
      "INFO 05-11 20:36:35 [gpu_model_runner.py:1686] Graph capturing finished in 34 secs, took 0.62 GiB\n",
      "INFO 05-11 20:36:35 [core.py:159] init engine (profile, create kv cache, warmup model) took 58.89 seconds\n",
      "INFO 05-11 20:36:35 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "size = 4096 - 1024\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    task=\"generate\",\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=size,\n",
    "    max_num_batched_tokens=size,\n",
    "    quantization=\"awq_marlin\",\n",
    "    gpu_memory_utilization=0.95,\n",
    "    # enforce_eager=True,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(**sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b6100",
   "metadata": {},
   "source": [
    "## Extract the main ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99477efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-11 20:36:37] INFO config.py:54: PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data_path = Path(\"../data\") / \"raw\" / \"contributions\"\n",
    "dataset = Dataset.load_from_disk(str(data_path.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ed16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rétablir l'ISF sans délai avec incorporation dans son assiette de tous les éléments de fortune (objets d'art inclus + yachts etc) sans aucune autre dérogation ni exception qu'une fraction (25%) de la résidence principale ET les sommes investies durablement (5 ans) dans une entreprise française pour une affectation en FRANCE\n"
     ]
    }
   ],
   "source": [
    "contribution = dataset[10]['contribution']\n",
    "print(contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ca6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rétablir l'ISF sans délai avec incorporation dans son assiette de tous les éléments de fortune (objets d'art inclus + yachts etc) sans aucune autre dérogation ni exception qu'une fraction (25%) de la résidence principale ET les sommes investies durablement (5 ans) dans une entreprise française pour une affectation en FRANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "38940d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROMPT_PATH, \"rb\") as file:\n",
    "    prompt_configs = tomllib.load(file)\n",
    "\n",
    "system_message = prompt_configs[\"prompt\"][\"system\"]\n",
    "user_message = prompt_configs[\"prompt\"][\"user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9a6422d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.39s/it, est. speed input: 12.66 toks/s, output: 3.71 toks/s]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message.format(input=contribution)},\n",
    "    {\"role\": \"assistant\", \"content\": \"description,type,syntax,semantic\"},\n",
    "]\n",
    "\n",
    "output = llm.chat(messages, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "15d5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "csv_data = output[0].outputs[0].text\n",
    "df = pd.read_csv(io.StringIO(csv_data), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4db9958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Rétablir l\\'ISF sans délai\",statement,positive,neutral\\n\"Incorporer dans son assiette de tous les éléments de fortune\",statement,positive,negative\\n\"Objets d\\'art inclus\",statement,positive,negative\\n\"Yachts etc\",statement,positive,negative\\n\"Une fraction (25%) de la résidence principale\",statement,negative,negative\\n\"Les sommes investies durablement (5 ans) dans une entreprise française\",statement,positive,negative\\n\"Affectation en FRANCE\",statement,positive,negative'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6cdb2653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rétablir l'ISF sans délai</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorporer dans son assiette de tous les éléme...</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Objets d'art inclus</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yachts etc</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Une fraction (25%) de la résidence principale</td>\n",
       "      <td>statement</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Les sommes investies durablement (5 ans) dans ...</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Affectation en FRANCE</td>\n",
       "      <td>statement</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1         2  \\\n",
       "0                          Rétablir l'ISF sans délai  statement  positive   \n",
       "1  Incorporer dans son assiette de tous les éléme...  statement  positive   \n",
       "2                                Objets d'art inclus  statement  positive   \n",
       "3                                         Yachts etc  statement  positive   \n",
       "4      Une fraction (25%) de la résidence principale  statement  negative   \n",
       "5  Les sommes investies durablement (5 ans) dans ...  statement  positive   \n",
       "6                              Affectation en FRANCE  statement  positive   \n",
       "\n",
       "          3  \n",
       "0   neutral  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  negative  \n",
       "4  negative  \n",
       "5  negative  \n",
       "6  negative  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519779",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Small LLM sometimes struggle to extract properly the main ideas from the original content.\n",
    "- the JSON format seems more stable but it costs more token so is more slow to be generated.\n",
    "\n",
    "Above, we've done the extraction for one contribution. Now, you can imagine to realize the extraction for thousands of contributions. Keep track of the extractions for each contribution to get the main statements and propositions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
